TITLE:: FluidNMFFilter
SUMMARY:: Realtime Non-Negative Matrix Factorisation with Fixed Bases
CATEGORIES:: Libraries>FluidCorpusManipulation
RELATED:: Classes/FluidNMFMatch,Classes/FluidBufNMF,Guides/FluidCorpusManipulation
DESCRIPTION::

    
    Decomposes and resynthesises a signal against a set of spectral templates


    
    This uses a slimmed-down version of Nonnegative Matrix Factorisation (NMF, see Lee, Daniel D., and H. Sebastian Seung. 1999. ‘Learning the Parts of Objects by Non-Negative Matrix Factorization’. Nature 401 (6755): 788–91. LINK::https://doi.org/10.1038/44565::)

    It outputs as a signal the resynthesis of the best factorisation. The spectral templates are presumed to have been produced by the offline NMF process, BufNMF, and must be the correct size with respect to the FFT settings being used (FFT size / 2 + 1 frames long). The components of the decomposition are determined by the number of channels in the supplied buffer of templates, up to a maximum set by the maxComponents parameter.

    NMF has been a popular technique in signal processing research for things like source separation and transcription (see e.g. Smaragdis and Brown, Non-Negative Matrix Factorization for Polyphonic Music Transcription.), although its creative potential is so far relatively unexplored. It works iteratively, by trying to find a combination of amplitudes ('activations') that yield the original magnitude spectrogram of the audio input when added together. By and large, there is no unique answer to this question (i.e. there are different ways of accounting for an evolving spectrum in terms of some set of templates and envelopes). In its basic form, NMF is a form of unsupervised learning: it starts with some random data and then converges towards something that minimizes the distance between its generated data and the original:it tends to converge very quickly at first and then level out. Fewer iterations mean less processing, but also less predictable results.

    The whole process can be related to a channel vocoder where, instead of fixed bandpass filters, we get more complex filter shapes and the activations correspond to channel envelopes.


Read more about FluidNMFFilter on the link::https://learn.flucoma.org/reference/nmffilter##learn platform::.

CLASSMETHODS::

METHOD:: ar

ARGUMENT:: in
The audio to be processed.








ARGUMENT:: bases

    
    The buffer containing the different bases that the input signal will be matched against. Bases must be (fft size / 2) + 1 frames. If the buffer has more than maxComponents channels, the excess will be ignored.



ARGUMENT:: maxComponents

    
    The maximum number of elements the NMF algorithm will try to divide the spectrogram of the source in. This dictates the number of output channels for the ugen. This cannot be modulated.

    STRONG::Constraints::

    LIST::
    ## 
    Minimum: CODE::1::

    ## 
    Maximum: CODE::maxmaxComponents::

    ::


ARGUMENT:: iterations

    
    The NMF process is iterative, trying to converge to the smallest error in its factorisation. The number of iterations will decide how many times it tries to adjust its estimates. Higher numbers here will be more CPU intensive, lower numbers will be more unpredictable in quality.

    STRONG::Constraints::

    LIST::
    ## 
    Minimum: CODE::1::

    ::


ARGUMENT:: windowSize

    
    The number of samples that are analysed at a time. A lower number yields greater temporal resolution, at the expense of spectral resolution, and vice-versa.



ARGUMENT:: hopSize

    
    The window hop size. As NMF relies on spectral frames, we need to move the window forward. It can be any size, but low overlap will create audible artefacts. The -1 default value will default to half of windowSize (overlap of 2).



ARGUMENT:: fftSize

    
    The FFT/IFFT size. It should be at least 4 samples long, at least the size of the window, and a power of 2. Making it larger allows an oversampling of the spectral precision. The -1 default value will default to windowSize.



ARGUMENT:: maxFFTSize

    
    Set an explicit upper bound on the FFT size at object instantiation. The default of CODE::nil:: or -1 sets this to whatever the initial FFT size is



ARGUMENT:: maxMaxComponents

    
    Manually sets a maximum value for CODE::maxComponents::. Can only be set at object instantiation. Default value of -1 sets this to the initial value of CODE::maxComponents::



 
 

INSTANCEMETHODS::
  
EXAMPLES::
code::
/* For information on NMF and the associated Bases, please see the */ FluidBufNMF /* helpfile. */

// We'll start by getting some bases for this drum loop:
(
~drums = Buffer.read(s,FluidFilesPath("Nicol-LoopE-M.wav"));
~resynth = Buffer(s);
~bases = Buffer(s);
~n_components = 3;
)

// Process it:
FluidBufNMF.processBlocking(s,~drums,resynth:~resynth,resynthMode:1,bases:~bases,components:~n_components,action:{"done".postln;});

// Once it is done, let's hear what components it was able to decompose:
// Play the separated components one by one (with a second of silence in between).
(
fork{
	~n_components.do{
		arg i;
		"decomposed part #%".format(i+1).postln;
		{
			PlayBuf.ar(~n_components,~resynth,BufRateScale.ir(~resynth),doneAction:2)[i].dup;
		}.play;
		(~drums.duration + 1).wait;
	}
};
)

// If the process did not separate out the kick drum, hi hat, and snare drum (in any order) very well, try it again until it does.
// It will be useful going forward to have bases that pretty well represent these drum instruments.

// FluidNMFFilter will use the bases (spectral templates) of a FluidBufNMF analysis to filter (i.e., decompose) realtime audio
// First, we'll just send the same drum loop through FluidNMFFilter to hear it in action:
(
fork{
	~n_components.do{
		arg i;
		"decomposed part #% filtered in realtime using FluidNMFFilter".format(i+1).postln;
		{
			var src = PlayBuf.ar(1,~drums,BufRateScale.ir(~drums),doneAction:2);
			var sig = FluidNMFFilter.ar(src,~bases,~n_components)[i];
			sig;
		}.play;
		(~drums.duration + 1).wait;
	}
};
)

/* It is seprating the drum instrument components just like it did with FluidBufNMF--the difference here is that it is
happening in realtime. */

// This means one could different FX to each instrument:

(
{
	var src = PlayBuf.ar(1,~drums,BufRateScale.ir(~drums),doneAction:2);
	var sig = FluidNMFFilter.ar(src,~bases,3);
	var mix;

	mix = Splay.ar(sig);

	mix = mix + CombN.ar(sig[2],0.1,[0.09,0.1],1.0);
	mix = mix + CombC.ar(sig[1],0.2,LFTri.kr(0.1).range(0.0,0.2));
	mix = mix + GVerb.ar(sig[0],100,7,0.9,drylevel:0,mul:-10.dbamp);

	mix;
}.play;
)

/* This also means that one could send a live input audio signal through FluidNMFFilter and get these
drum instruments similarly decomposed into separate audio streams in *realtime*. */

/* To test an idea like this, we could train it on just the first few seconds of this drum loop (like a training set when using
a neural network) and then see how it peforms on audio it hasn't seen before: the rest of the drum loop (similar to using
a testing set when working with a neural network) */

(
~split_point_seconds = 3;
// load a training buffer with only the first part of the drum loop:
~drums_training = Buffer.read(s,FluidFilesPath("Nicol-LoopE-M.wav"),numFrames:~drums.sampleRate * ~split_point_seconds);
// load a testing buffer with the rest:
~drums_testing = Buffer.read(s,FluidFilesPath("Nicol-LoopE-M.wav"),startFrame:~drums.sampleRate * ~split_point_seconds);
~bases_from_training = Buffer(s);
)

// Process it:
FluidBufNMF.processBlocking(s,~drums_training,bases:~bases_from_training,components:3,action:{"done".postln;});

// Hear it:
(
fork{
	3.do{
		arg i;
		"decomposed part #% filtered in realtime using FluidNMFFilter\n\tIt has never heard this part of the drum loop!\n".format(i+1).postln;
		{
			var src = PlayBuf.ar(1,~drums_testing,BufRateScale.ir(~drums_testing),doneAction:2);
			var sig = FluidNMFFilter.ar(src,~bases_from_training,3)[i];
			sig;
		}.play;
		(~drums_testing.duration + 1).wait;
	}
};
)

// If we play a different source through FluidNMFFilter, it will try to decompose that realtime signal according to the bases
// it is given (in our case the bases from the drum loop)

~song = Buffer.readChannel(s,FluidFilesPath("Tremblay-BeatRemember.wav"),channels:[0]);

(
fork{
	~n_components.do{
		arg i;
		"decomposed part #% filtered in realtime using FluidNMFFilter".format(i+1).postln;
		{
			var src = PlayBuf.ar(1,~song,BufRateScale.ir(~song),doneAction:2);
			var sig = FluidNMFFilter.ar(src,~bases,~n_components)[i];
			sig;
		}.play;
		(~song.duration + 1).wait;
	}
};
)

/* FluidNMFFilter has pretty well been able to identify and filter the kick drum, snare drum, and hi hat in this song
using on the corresponding bases from the ~drums analysis. It isn't perfect because it wasn't trained on these
specific drum sounds and how they exist in this ensemble recording. */

::
STRONG::A guitar processor::
CODE::
//set some buffers
(
~guitar = Buffer.read(s,FluidFilesPath("Tremblay-AaS-AcousticStrums-M.wav"));
~resynth = Buffer.new(s);
~bases = Buffer.new(s);
~spectralshapes = Buffer.new(s);
~stats = Buffer.new(s);
~trainedBases = Buffer.new(s);
~n_components = 10;
~centroids = Array.newClear(~n_components);
)

// hear the guitar
~guitar.play;

// what we're going to try to achieve here is to use FluidNMFFilter to separate out the pick from the strings and do fx processing on them separately.

// train only 2 seconds, asking for 10 components
FluidBufNMF.process(s,~guitar,0,88200,resynth:~resynth, resynthMode:1, bases:~bases, components:~n_components,fftSize:2048,action:{"done".postln;});

// We're going to find the component that has the picking sound by doing a FluidSpectralShape analysis on each of the resynthesized components, then use FluidBufStats to get the median spectral centroid of each component.
(
fork{
	~n_components.do{
		arg i;
		// we'll do the spectral shape of the ~resynth buffer one channel at a time:
		FluidBufSpectralShape.processBlocking(s,~resynth,startChan:i,numChans:1,features:~spectralshapes);
		// telling FluidBufStats numChans = 1, means it will only analyze channel 0, which is spectral centroid
		FluidBufStats.processBlocking(s,~spectralshapes,numChans:1,stats:~stats);
		~stats.loadToFloatArray(action:{
			arg stats;
			~centroids[i] = stats[5]; // the median spectral centroid will be in index 5
		});
	};
	s.sync;
	~centroids.postln;
}
)

// Now we're going to create a new, 2 channel buffer to use as the bases for FluidNMFFilter. First we'll copy the basis with the lowest median centroid to channel 0, and all the *other* bases to channel 1. The hope here is that the base created for the resynthesized sound with the lowest median centroid will contain the sustain of the guitar strings, while all the others will contain components related to the "pick" part of the sound

(
~minIndex = ~centroids.minIndex;
~n_components.do{
	arg i;
	if(i == ~minIndex,{
		"base % has the lowest centroid".format(i).postln;
		FluidBufCompose.processBlocking(s,~bases,startChan:i,numChans:1,destination:~trainedBases,destStartChan:0);
	},{
		FluidBufCompose.processBlocking(s,~bases,startChan:i,numChans:1,destination:~trainedBases,destStartChan:1,destGain:1);
	});
};
)

~trainedBases.plot;

//we can then use the  signal to send to a delay
(
{
	var source, todelay, delay1, delay2, delay3, feedback, mod1, mod2, mod3, mod4;
	//read the source
	source = PlayBuf.ar(1, ~guitar);

	// generate modulators that are coprime in frequency
	mod1 = SinOsc.ar(1, 0, 0.001);
	mod2 = SinOsc.ar(((617 * 181) / (461 * 991)), 0, 0.001);
	mod3 = SinOsc.ar(((607 * 193) / (491 * 701)), 0, 0.001);
	mod4 = SinOsc.ar(((613 * 191) / (463 * 601)), 0, 0.001);

	// the signal to send to the delays
	todelay = FluidNMFFilter.ar(source,~trainedBases,2,fftSize:2048)[0]; //reading the channel of the activations on the pick basis

	// delay network
	feedback = LocalIn.ar(3);// take the feedback in for the delays
	delay1 = DelayC.ar(BPF.ar(todelay+feedback[1]+(feedback[2] * 0.3), 987, 6.7,0.8),0.123,0.122+(mod1*mod2));
	delay2 = DelayC.ar(BPF.ar(todelay+feedback[0]+(feedback[2] * 0.3), 1987, 6.7,0.8),0.345,0.344+(mod3*mod4));
	delay3 = DelayC.ar(BPF.ar(todelay+feedback[1], 1456, 6.7,0.8),0.567,0.566+(mod1*mod3),0.6);
	LocalOut.ar([delay1,delay2, delay3]); // write the feedback for the delays

	source.dup + ([delay1+delay3,delay2+delay3]*(3.dbamp));
	//listen to the delays in solo by uncommenting the following line
	// [delay1+delay3,delay2+delay3]
	// [source, todelay]
}.play;
)
::
